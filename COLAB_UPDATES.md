# üîß UPDATED CONFIGURATION FOR GOOGLE COLAB
# Copy and paste this into Cell 4 of your notebook

```python
# Configuration - UPDATED VERSION
class Config:
    # Paths - MODIFY THESE FOR YOUR COLAB SETUP
    YOLO_WEIGHTS = '/content/best.pt'  # Upload your trained YOLO model
    SAM_WEIGHTS = '/content/sam3.pt'   # Upload SAM 3 weights (optional)
    TEST_IMAGES_DIR = '/content/images/test'  # Test images directory
    GROUND_TRUTH_DIR = '/content/labels/test'  # Ground truth annotations
    OUTPUT_DIR = '/content/results'
    
    # Detection parameters - OPTIMIZED
    CONFIDENCE_THRESHOLD = 0.25  # Lower threshold to detect more objects
    IOU_THRESHOLD = 0.5  # Standard IoU threshold
    SAM_IMAGE_SIZE = 1024
    
    # ‚ö†Ô∏è CRITICAL FIX: Proper class definitions
    CLASS_NAMES = {
        0: 'helmet', 1: 'gloves', 2: 'vest', 3: 'boots',
        4: 'goggles', 5: 'none', 6: 'Person', 7: 'no_helmet',
        8: 'no_goggle', 9: 'no_gloves'
    }
    
    # Evaluation categories (what to measure)
    PERSON_CLASS = [6]  # Person detection
    PPE_CLASSES = [0, 1, 2, 3, 4]  # All PPE items
    VIOLATION_CLASSES = [7, 8, 9]  # All violations
    
    # Focus on these key items for evaluation
    KEY_CLASSES = {
        'person': [6],
        'helmet': [0],
        'vest': [2],
        'no_helmet': [7],
        'gloves': [1]
    }
    
    # ROI parameters
    HEAD_ROI_RATIO = 0.4
    TORSO_START_RATIO = 0.2
    TORSO_END_RATIO = 0.7

config = Config()
os.makedirs(config.OUTPUT_DIR, exist_ok=True)
print("‚öôÔ∏è Configuration loaded")
print(f"üìä Evaluation will cover:")
print(f"   - Person detection (Class {config.PERSON_CLASS})")
print(f"   - PPE items: helmet={config.KEY_CLASSES['helmet']}, vest={config.KEY_CLASSES['vest']}")
print(f"   - Violations: no_helmet={config.KEY_CLASSES['no_helmet']}")
```

---

# üìä KEY INSIGHTS FROM YOUR RESULTS

## What the Diagnosis Revealed:

### Ground Truth Distribution (141 test images):
- **236 Person instances** (18.86%)
- **192 Helmet instances** (15.35%)
- **178 Vest instances** (14.23%)
- **40 No_helmet violations** (3.20%) ‚ö†Ô∏è RARE CLASS!
- **211 Boots, 163 Gloves, 52 Goggles** (other PPE)

### Current Results Problem:
```json
{
    "yolo_only": {"tp": 0, "fp": 201, "fn": 40},  // 0% accuracy!
    "hybrid": {"tp": 0, "fp": 55, "fn": 40}       // 0% accuracy!
}
```

**BUT: SAM reduced False Positives by 72.6%!** (201 ‚Üí 55) ‚úÖ

### Why 0 True Positives?
Your original config was ONLY looking for **violations (no_helmet: class 7)** but:
1. Ground truth has only **40 no_helmet instances** (very rare!)
2. Class matching might be failing
3. IOU threshold (0.3) + confidence (0.4) might be too strict

---

# üéØ RECOMMENDED APPROACH

## Option 1: Evaluate PPE Detection Performance (RECOMMENDED)

This measures what your model ACTUALLY detects well:

```python
# Add this as a NEW cell after utilities

def evaluate_multi_category(model, test_dir, gt_dir):
    """
    Comprehensive evaluation across multiple categories:
    - Person detection
    - Helmet detection  
    - Vest detection
    - Violation detection (no_helmet)
    """
    results = {
        'person': {'tp': 0, 'fp': 0, 'fn': 0},
        'helmet': {'tp': 0, 'fp': 0, 'fn': 0},
        'vest': {'tp': 0, 'fp': 0, 'fn': 0},
        'no_helmet': {'tp': 0, 'fp': 0, 'fn': 0},
        'overall_ppe': {'tp': 0, 'fp': 0, 'fn': 0}
    }
    
    image_files = glob.glob(os.path.join(test_dir, '*.jpg'))
    
    for img_path in image_files:
        img = cv2.imread(img_path)
        if img is None:
            continue
        
        h, w = img.shape[:2]
        img_name = os.path.basename(img_path)
        gt_path = os.path.join(gt_dir, img_name.replace('.jpg', '.txt'))
        
        # Load ground truth
        gt_annotations = parse_yolo_annotation(gt_path, w, h)
        
        # Run detection
        detections = model.predict(img_path, conf=config.CONFIDENCE_THRESHOLD, verbose=False)[0]
        
        # Separate by category
        categories = {
            'person': (config.KEY_CLASSES['person'], []),
            'helmet': (config.KEY_CLASSES['helmet'], []),
            'vest': (config.KEY_CLASSES['vest'], []),
            'no_helmet': (config.KEY_CLASSES['no_helmet'], [])
        }
        
        # Group detections
        for box in detections.boxes:
            cls = int(box.cls[0])
            coords = box.xyxy[0].cpu().numpy()
            bbox = [coords[0], coords[1], coords[2], coords[3]]
            
            for cat_name, (class_ids, det_list) in categories.items():
                if cls in class_ids:
                    det_list.append({'class': cls, 'bbox': bbox})
        
        # Group ground truth
        gt_by_category = {cat: [] for cat in categories.keys()}
        for gt in gt_annotations:
            cls_id = gt[0]
            bbox = gt[1:5]
            for cat_name, (class_ids, _) in categories.items():
                if cls_id in class_ids:
                    gt_by_category[cat_name].append({'class': cls_id, 'bbox': bbox})
        
        # Calculate TP/FP/FN for each category
        for cat_name in categories.keys():
            dets = categories[cat_name][1]
            gts = gt_by_category[cat_name]
            
            tp, fp, fn = match_detections(dets, gts, config.IOU_THRESHOLD)
            results[cat_name]['tp'] += tp
            results[cat_name]['fp'] += fp
            results[cat_name]['fn'] += fn
    
    # Calculate metrics
    metrics = {}
    for cat_name, counts in results.items():
        tp, fp, fn = counts['tp'], counts['fp'], counts['fn']
        precision, recall, f1 = calculate_metrics(tp, fp, fn)
        
        metrics[cat_name] = {
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'tp': tp, 'fp': fp, 'fn': fn
        }
    
    return metrics


def match_detections(detections, ground_truths, iou_threshold):
    """Match detections to ground truth and count TP/FP/FN"""
    if len(ground_truths) == 0:
        return 0, len(detections), 0
    
    if len(detections) == 0:
        return 0, 0, len(ground_truths)
    
    matched_gt = set()
    tp = 0
    
    for det in detections:
        best_iou = 0
        best_gt_idx = -1
        
        for idx, gt in enumerate(ground_truths):
            if idx in matched_gt:
                continue
            
            iou = calculate_iou(det['bbox'], gt['bbox'])
            if iou > best_iou and iou >= iou_threshold:
                best_iou = iou
                best_gt_idx = idx
        
        if best_gt_idx >= 0:
            tp += 1
            matched_gt.add(best_gt_idx)
    
    fp = len(detections) - tp
    fn = len(ground_truths) - tp
    
    return tp, fp, fn


print("‚úÖ Multi-category evaluation functions loaded")
```

Then run evaluation:

```python
# Run comprehensive evaluation
print("üîç Running YOLO-Only Evaluation...")
yolo_model = YOLO(config.YOLO_WEIGHTS)
yolo_metrics = evaluate_multi_category(yolo_model, config.TEST_IMAGES_DIR, config.GROUND_TRUTH_DIR)

# Print results
print("\n" + "="*60)
print("YOLO-ONLY RESULTS BY CATEGORY")
print("="*60)

for category, metrics in yolo_metrics.items():
    print(f"\n{category.upper().replace('_', ' ')}:")
    print(f"  Precision: {metrics['precision']:.3f}")
    print(f"  Recall:    {metrics['recall']:.3f}")
    print(f"  F1-Score:  {metrics['f1_score']:.3f}")
    print(f"  TP: {metrics['tp']}, FP: {metrics['fp']}, FN: {metrics['fn']}")

# Save results
with open(os.path.join(config.OUTPUT_DIR, 'category_results.json'), 'w') as f:
    json.dump(yolo_metrics, f, indent=4)

print(f"\n‚úÖ Results saved to {config.OUTPUT_DIR}/category_results.json")
```

---

## Option 2: Focus on Violation Detection Only

If you want to focus ONLY on the 40 no_helmet violations:

```python
# Simplified evaluation - no_helmet violations only
def evaluate_violations_only():
    model = YOLO(config.YOLO_WEIGHTS)
    
    tp, fp, fn = 0, 0, 0
    
    image_files = glob.glob(os.path.join(config.TEST_IMAGES_DIR, '*.jpg'))
    
    for img_path in image_files:
        img = cv2.imread(img_path)
        h, w = img.shape[:2]
        
        # Load GT
        gt_path = os.path.join(config.GROUND_TRUTH_DIR, 
                               os.path.basename(img_path).replace('.jpg', '.txt'))
        gt_annotations = parse_yolo_annotation(gt_path, w, h)
        gt_violations = [gt for gt in gt_annotations 
                        if gt[0] in config.KEY_CLASSES['no_helmet']]
        
        # Detect
        results = model.predict(img_path, conf=config.CONFIDENCE_THRESHOLD, verbose=False)[0]
        det_violations = []
        
        for box in results.boxes:
            if int(box.cls[0]) in config.KEY_CLASSES['no_helmet']:
                coords = box.xyxy[0].cpu().numpy()
                det_violations.append({'bbox': [coords[0], coords[1], coords[2], coords[3]]})
        
        # Match
        img_tp, img_fp, img_fn = match_detections(det_violations, 
                                                   [{'bbox': gt[1:5]} for gt in gt_violations],
                                                   config.IOU_THRESHOLD)
        tp += img_tp
        fp += img_fp
        fn += img_fn
    
    precision, recall, f1 = calculate_metrics(tp, fp, fn)
    
    print(f"\nüéØ NO_HELMET VIOLATION DETECTION:")
    print(f"   Precision: {precision:.3f}")
    print(f"   Recall: {recall:.3f}")
    print(f"   F1-Score: {f1:.3f}")
    print(f"   TP={tp}, FP={fp}, FN={fn}")
    
    return {'precision': precision, 'recall': recall, 'f1_score': f1, 
            'tp': tp, 'fp': fp, 'fn': fn}

violation_results = evaluate_violations_only()
```

---

# üì¶ COMPLETE CELL-BY-CELL UPDATES

## Replace Cell 4 (Configuration):
Use the new Config class at the top of this document

## Add NEW Cell after utilities (Cell 6):
```python
# Multi-category evaluation function
[Copy the evaluate_multi_category function from Option 1 above]
```

## Replace evaluation cell:
```python
# Run comprehensive evaluation
print("üîç Evaluating detection performance across categories...")
yolo_model = YOLO(config.YOLO_WEIGHTS)
results = evaluate_multi_category(yolo_model, config.TEST_IMAGES_DIR, config.GROUND_TRUTH_DIR)

# Display and save
for category, metrics in results.items():
    print(f"\n{category.upper()}: P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}")

with open(os.path.join(config.OUTPUT_DIR, 'fixed_results.json'), 'w') as f:
    json.dump(results, f, indent=4)
```

---

# üöÄ QUICK START FOR COLAB

1. **Upload files to Colab:**
   ```
   /content/
   ‚îú‚îÄ‚îÄ best.pt (your YOLO model)
   ‚îú‚îÄ‚îÄ images/test/ (141 test images)
   ‚îî‚îÄ‚îÄ labels/test/ (141 annotation files)
   ```

2. **Replace Cell 4** with new Config

3. **Add multi-category evaluation function** after utilities

4. **Run evaluation** with the new approach

5. **Expected better results:**
   - Person detection: High F1 (persons are common)
   - Helmet detection: Moderate F1 (192 instances)
   - Vest detection: Moderate F1 (178 instances)
   - No_helmet: Lower F1 (only 40 instances, rare class)

---

# üí° WHY THIS FIXES THE PROBLEM

**Before:** Only looking for no_helmet violations (40 instances, 3.2% of data)
**After:** Evaluating ALL categories (person, helmet, vest, violations)

This gives you:
‚úÖ Meaningful metrics for each class
‚úÖ Understanding of what model detects well
‚úÖ Proper comparison between YOLO-only and Hybrid
‚úÖ Evidence for your research paper

The key insight: **Your model detects PPE PRESENCE well, not just violations!**
