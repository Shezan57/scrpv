

================================================================================
=== Hierarchical_Decision_and_Agentic_System_(YOLO_+_SAM_3_+_Agent).ipynb ===
================================================================================

--- CELL 4 ---
# @title 3. Phase 1: Hierarchical Detection System
from ultralytics import YOLO
from ultralytics.models.sam import SAM3SemanticPredictor

class SafetyDetector:
    def __init__(self):
        print("üöÄ Initializing Hierarchical Detection System...")
        self.yolo_model = YOLO(config.YOLO_WEIGHTS)

        # Load SAM 3
        overrides = dict(model=config.SAM_WEIGHTS, task="segment", mode="predict", conf=0.15)
        self.sam_model = SAM3SemanticPredictor(overrides=overrides)

        # Update Class IDs dynamically if possible
        names = self.yolo_model.names
        name_to_id = {v: k for k, v in names.items()}
        for key in config.TARGET_CLASSES:
            if key in name_to_id: config.TARGET_CLASSES[key] = [name_to_id[key]] # Fallback to strict map if key name differs

    def box_iou(self, box1, box2):
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        inter = max(0, x2 - x1) * max(0, y2 - y1)
        if inter == 0: return 0
        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
        return inter / box2_area

    def run_sam_rescue(self, image_path, search_prompts, roi_box, h, w):
        """Runs SAM only on ROI"""
        try:
            res = self.sam_model(image_path, text=search_prompts, imgsz=config.SAM_IMAGE_SIZE, verbose=False)
            if not res[0].masks: return False
            masks = [m.cpu().numpy().astype(np.uint8) for m in res[0].masks.data]
            for m in masks:
                if m.shape[:2] != (h, w): m = cv2.resize(m, (w, h), interpolation=cv2.INTER_NEAREST)
                roi = m[roi_box[1]:roi_box[3], roi_box[0]:roi_box[2]]
                if np.sum(roi) > 0: return True
        except: pass
        return False

    def detect(self, image_path):
        img = cv2.imread(image_path)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        h, w = img_rgb.shape[:2]

        # --- YOLO SCAN ---
        results = self.yolo_model.predict(image_path, conf=config.CONFIDENCE_THRESHOLD, verbose=False)
        detections = {'person': [], 'helmet': [], 'vest': [], 'no_helmet': []}

        for box in results[0].boxes:
            cls = int(box.cls[0])
            coords = box.xyxy[0].cpu().numpy().astype(int)
            # Map YOLO classes to our buckets
            for key, ids in config.TARGET_CLASSES.items():
                if cls in ids: detections[key].append(coords)

        violations = []

        # --- HIERARCHICAL LOGIC ---
        for p_box in detections['person']:
            has_helmet, has_vest, unsafe_explicit = False, False, False

            # Check Overlaps
            for eq in detections['helmet']:
                if self.box_iou(p_box, eq) > 0.3: has_helmet = True
            for eq in detections['vest']:
                if self.box_iou(p_box, eq) > 0.3: has_vest = True
            for eq in detections['no_helmet']:
                if self.box_iou(p_box, eq) > 0.3: unsafe_explicit = True

            status = "SAFE"
            missing = []

            # 1. Fast Unsafe
            if unsafe_explicit:
                status = "UNSAFE"
                missing.append("Helmet")

            # 2. Fast Safe
            elif has_helmet and has_vest:
                status = "SAFE"

            # 3. Rescue Vest
            elif has_helmet and not has_vest:
                body_roi = [p_box[0], int(p_box[1] + (p_box[3]-p_box[1])*0.2), p_box[2], p_box[3]]
                if not self.run_sam_rescue(image_path, ["vest"], body_roi, h, w):
                    status = "UNSAFE"
                    missing.append("Vest")

            # 4. Rescue Helmet
            elif has_vest and not has_helmet:
                head_roi = [p_box[0], p_box[1], p_box[2], int(p_box[1] + (p_box[3]-p_box[1])*0.4)]
                if not self.run_sam_rescue(image_path, ["helmet"], head_roi, h, w):
                    status = "UNSAFE"
                    missing.append("Helmet")

            # 5. Full Rescue
            else:
                head_roi = [p_box[0], p_box[1], p_box[2], int(p_box[1] + (p_box[3]-p_box[1])*0.4)]
                body_roi = [p_box[0], int(p_box[1] + (p_box[3]-p_box[1])*0.2), p_box[2], p_box[3]]
                found_h = self.run_sam_rescue(image_path, ["helmet"], head_roi, h, w)
                found_v = self.run_sam_rescue(image_path, ["vest"], body_roi, h, w)

                if not found_h or not found_v:
                    status = "UNSAFE"
                    if not found_h: missing.append("Helmet")
                    if not found_v: missing.append("Vest")

            # --- LOG VIOLATION IF UNSAFE ---
            if status == "UNSAFE":
                timestamp = datetime.now()
                # Draw box on copy of image for report
                evidence_img = img_rgb.copy()
                cv2.rectangle(evidence_img, (p_box[0], p_box[1]), (p_box[2], p_box[3]), (255, 0, 0), 3)

                evidence_path = f"{config.VIOLATIONS_DIR}/violation_{timestamp.strftime('%H%M%S')}.jpg"
                cv2.imwrite(evidence_path, cv2.cvtColor(evidence_img, cv2.COLOR_RGB2BGR))

                violation_data = {
                    "timestamp": timestamp,
                    "location": config.SITE_LOCATION,
                    "description": f"Worker detected without {', '.join(missing)}",
                    "missing_items": missing,
                    "confidence": 0.85, # Aggregate confidence
                    "image_path": evidence_path,
                    "bbox": p_box
                }
                violations.append(violation_data)

        return violations

detector = SafetyDetector()
print("‚úÖ Detector Ready")

--- CELL 5 ---
# @title 3.1 Enhanced Detector with Path Tracking
from collections import Counter
import glob

class EnhancedSafetyDetector(SafetyDetector):
    """Extended detector that tracks decision paths for analysis"""
    
    def __init__(self):
        super().__init__()
        self.decision_paths = []
    
    def detect_with_tracking(self, image_path):
        """Run detection and track which decision paths are taken"""
        img = cv2.imread(image_path)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        h, w = img_rgb.shape[:2]

        # --- YOLO SCAN ---
        results = self.yolo_model.predict(image_path, conf=config.CONFIDENCE_THRESHOLD, verbose=False)
        detections = {'person': [], 'helmet': [], 'vest': [], 'no_helmet': []}

        for box in results[0].boxes:
            cls = int(box.cls[0])
            coords = box.xyxy[0].cpu().numpy().astype(int)
            for key, ids in config.TARGET_CLASSES.items():
                if cls in ids: detections[key].append(coords)

        violations = []
        frame_paths = []

        # --- HIERARCHICAL LOGIC WITH PATH TRACKING ---
        for p_box in detections['person']:
            has_helmet, has_vest, unsafe_explicit = False, False, False

            # Check Overlaps
            for eq in detections['helmet']:
                if self.box_iou(p_box, eq) > 0.3: has_helmet = True
            for eq in detections['vest']:
                if self.box_iou(p_box, eq) > 0.3: has_vest = True
            for eq in detections['no_helmet']:
                if self.box_iou(p_box, eq) > 0.3: unsafe_explicit = True

            status = "SAFE"
            missing = []
            decision_path = ""

            # 1. Fast Unsafe (Fast Violation Path)
            if unsafe_explicit:
                status = "UNSAFE"
                missing.append("Helmet")
                decision_path = "Fast Violation"

            # 2. Fast Safe
            elif has_helmet and has_vest:
                status = "SAFE"
                decision_path = "Fast Safe"

            # 3. Rescue Vest (Rescue Body Path)
            elif has_helmet and not has_vest:
                decision_path = "Rescue Body"
                body_roi = [p_box[0], int(p_box[1] + (p_box[3]-p_box[1])*0.2), p_box[2], p_box[3]]
                if not self.run_sam_rescue(image_path, ["vest"], body_roi, h, w):
                    status = "UNSAFE"
                    missing.append("Vest")

            # 4. Rescue Helmet (Rescue Head Path)
            elif has_vest and not has_helmet:
                decision_path = "Rescue Head"
                head_roi = [p_box[0], p_box[1], p_box[2], int(p_box[1] + (p_box[3]-p_box[1])*0.4)]
                if not self.run_sam_rescue(image_path, ["helmet"], head_roi, h, w):
                    status = "UNSAFE"
                    missing.append("Helmet")

            # 5. Full Rescue (Critical Path)
            else:
                decision_path = "Critical"
                head_roi = [p_box[0], p_box[1], p_box[2], int(p_box[1] + (p_box[3]-p_box[1])*0.4)]
                body_roi = [p_box[0], int(p_box[1] + (p_box[3]-p_box[1])*0.2), p_box[2], p_box[3]]
                found_h = self.run_sam_rescue(image_path, ["helmet"], head_roi, h, w)
                found_v = self.run_sam_rescue(image_path, ["vest"], body_roi, h, w)

                if not found_h or not found_v:
                    status = "UNSAFE"
                    if not found_h: missing.append("Helmet")
                    if not found_v: missing.append("Vest")

            # Track the decision path
            frame_paths.append(decision_path)

            # --- LOG VIOLATION IF UNSAFE ---
            if status == "UNSAFE":
                timestamp = datetime.now()
                evidence_img = img_rgb.copy()
                cv2.rectangle(evidence_img, (p_box[0], p_box[1]), (p_box[2], p_box[3]), (255, 0, 0), 3)

                evidence_path = f"{config.VIOLATIONS_DIR}/violation_{timestamp.strftime('%H%M%S')}.jpg"
                cv2.imwrite(evidence_path, cv2.cvtColor(evidence_img, cv2.COLOR_RGB2BGR))

                violation_data = {
                    "timestamp": timestamp,
                    "location": config.SITE_LOCATION,
                    "description": f"Worker detected without {', '.join(missing)}",
                    "missing_items": missing,
                    "confidence": 0.85,
                    "image_path": evidence_path,
                    "bbox": p_box,
                    "decision_path": decision_path
                }
                violations.append(violation_data)

        # Store paths for this frame
        self.decision_paths.extend(frame_paths)
        
        return violations, frame_paths

# Initialize enhanced detector
enhanced_detector = EnhancedSafetyDetector()
print("‚úÖ Enhanced Detector with Path Tracking Ready")


================================================================================
=== Quantitative_SAM_Improvement_Analysis_colab.ipynb ===
================================================================================

--- CELL 10 ---
# Hybrid Detector (YOLO + SAM)

class HybridDetector:
    def __init__(self):
        print("üîß Initializing YOLO + SAM Hybrid System...")
        self.yolo_model = YOLO(config.YOLO_WEIGHTS)
        overrides = dict(model=config.SAM_WEIGHTS, task="segment", mode="predict", conf=0.15)
        self.sam_model = SAM3SemanticPredictor(overrides=overrides)
        print("‚úÖ Hybrid System Ready")

    def box_iou(self, box1, box2):
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        inter = max(0, x2 - x1) * max(0, y2 - y1)
        if inter == 0:
            return 0
        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
        return inter / box2_area

    def run_sam_rescue(self, image_path, search_prompts, roi_box, h, w):
        try:
            res = self.sam_model(image_path, text=search_prompts, imgsz=config.SAM_IMAGE_SIZE, verbose=False)
            if not res[0].masks:
                return False
            masks = [m.cpu().numpy().astype(np.uint8) for m in res[0].masks.data]
            for m in masks:
                if m.shape[:2] != (h, w):
                    m = cv2.resize(m, (w, h), interpolation=cv2.INTER_NEAREST)
                roi = m[roi_box[1]:roi_box[3], roi_box[0]:roi_box[2]]
                if np.sum(roi) > 0:
                    return True
        except:
            pass
        return False

    def evaluate_violations(self, image_path):
        img = cv2.imread(image_path)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        h, w = img_rgb.shape[:2]

        results = self.yolo_model.predict(image_path, conf=config.CONFIDENCE_THRESHOLD, verbose=False)
        detections = {'person': [], 'helmet': [], 'vest': [], 'no_helmet': []}

        for box in results[0].boxes:
            cls = int(box.cls[0])
            coords = box.xyxy[0].cpu().numpy().astype(int)
            for key, ids in config.TARGET_CLASSES.items():
                if cls in ids:
                    detections[key].append(coords)

        violations = []

        for p_box in detections['person']:
            has_helmet, has_vest, unsafe_explicit = False, False, False
            decision_path = ""

            for eq in detections['helmet']:
                if self.box_iou(p_box, eq) > 0.3:
                    has_helmet = True
            for eq in detections['vest']:
                if self.box_iou(p_box, eq) > 0.3:
                    has_vest = True
            for eq in detections['no_helmet']:
                if self.box_iou(p_box, eq) > 0.3:
                    unsafe_explicit = True

            if unsafe_explicit:
                decision_path = "Fast Violation"
                has_helmet = False
            elif has_helmet and has_vest:
                decision_path = "Fast Safe"
            elif has_helmet and not has_vest:
                decision_path = "Rescue Body"
                body_roi = [p_box[0], int(p_box[1] + (p_box[3]-p_box[1])*0.2), p_box[2], p_box[3]]
                has_vest = self.run_sam_rescue(image_path, ["vest"], body_roi, h, w)
            elif has_vest and not has_helmet:
                decision_path = "Rescue Head"
                head_roi = [p_box[0], p_box[1], p_box[2], int(p_box[1] + (p_box[3]-p_box[1])*0.4)]
                has_helmet = self.run_sam_rescue(image_path, ["helmet"], head_roi, h, w)
            else:
                decision_path = "Critical"
                head_roi = [p_box[0], p_box[1], p_box[2], int(p_box[1] + (p_box[3]-p_box[1])*0.4)]
                body_roi = [p_box[0], int(p_box[1] + (p_box[3]-p_box[1])*0.2), p_box[2], p_box[3]]
                has_helmet = self.run_sam_rescue(image_path, ["helmet"], head_roi, h, w)
                has_vest = self.run_sam_rescue(image_path, ["vest"], body_roi, h, w)

            is_violation = not has_helmet or not has_vest

            violations.append({
                'bbox': p_box,
                'has_helmet': has_helmet,
                'has_vest': has_vest,
                'is_violation': is_violation,
                'decision_path': decision_path,
                'confidence': 0.85
            })

        return violations

print("‚úÖ HybridDetector class defined")


================================================================================
=== yolo11m_sam3_hybrid_detection.ipynb ===
================================================================================

--- CELL 23 ---
# @title Smart "Conditional" Safety Pipeline (YOLO First -> SAM Fallback)
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO
from ultralytics.models.sam import SAM3SemanticPredictor
import sys

# ==========================================
# 1. SETUP
# ==========================================
IMAGE_PATH = '/content/female-construction-worker-without-helmet-260nw-2567321355.webp'
YOLO_WEIGHTS = '/content/best.pt'
SAM_WEIGHTS = "/content/sam3.pt"
SAM_IMAGE_SIZE = 1024  # Prevent CUDA crashes

print(f"üöÄ Initializing Smart Pipeline...")

# Load YOLO
try:
    yolo_model = YOLO(YOLO_WEIGHTS)
    names = yolo_model.names
    print(f"üìã Model Classes: {names}")
except Exception as e:
    sys.exit(f"‚ùå YOLO Error: {e}")

# Load SAM (But don't use it yet)
overrides = dict(model=SAM_WEIGHTS, task="segment", mode="predict", conf=0.15)
sam_model = SAM3SemanticPredictor(overrides=overrides)

# Get Class IDs
class_map = {name: id for id, name in names.items()}
ID_PERSON = class_map.get('Person', class_map.get('person'))
ID_HELMET = class_map.get('helmet')
ID_VEST = class_map.get('vest')

if ID_PERSON is None: print("‚ö†Ô∏è Warning: 'Person' class not found. Logic might fail.")

# ==========================================
# 2. HELPER FUNCTIONS
# ==========================================
def box_iou(box1, box2):
    """Checks if box2 is inside or overlaps significantly with box1"""
    # Simple intersection check
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    inter_area = max(0, x2 - x1) * max(0, y2 - y1)
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    # If intersection covers > 30% of the small box (PPE), we count it
    if box2_area == 0: return 0
    return inter_area / box2_area

def get_sam_masks(image_path, text_prompts):
    """Runs SAM safely with correct resolution"""
    masks = []
    try:
        # Use 1024 to avoid stride crash
        res = sam_model(image_path, text=text_prompts, imgsz=SAM_IMAGE_SIZE, verbose=False)
        if res[0].masks:
            masks = [m.cpu().numpy().astype(np.uint8) for m in res[0].masks.data]
    except Exception as e:
        print(f"   ‚ö†Ô∏è SAM Error: {e}")
    return masks

# ==========================================
# 3. SMART PIPELINE LOGIC
# ==========================================
def run_smart_pipeline(image_path):
    # Load Image
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    h, w = img_rgb.shape[:2]

    # --- STEP 1: YOLO DETECTION ---
    print("üëÄ Phase 1: YOLO Detection...")
    results = yolo_model.predict(image_path, conf=0.4, verbose=False)

    persons = []
    yolo_helmets = []
    yolo_vests = []

    # Sort detections by class
    for box in results[0].boxes:
        cls = int(box.cls[0])
        coords = box.xyxy[0].cpu().numpy().astype(int)

        if cls == ID_PERSON: persons.append(coords)
        elif cls == ID_HELMET: yolo_helmets.append(coords)
        elif cls == ID_VEST: yolo_vests.append(coords)

    print(f"   üëâ Found {len(persons)} Persons, {len(yolo_helmets)} Helmets, {len(yolo_vests)} Vests.")

    # --- STEP 2: CHECK YOLO COMPLETENESS ---
    final_results = []
    sam_triggered = False

    # SAM Cache (Run only once if needed)
    sam_helmet_masks = None
    sam_vest_masks = None

    for p_box in persons:
        # Check what YOLO found for THIS person
        has_helmet = False
        has_vest = False

        # Check overlaps
        for h_box in yolo_helmets:
            if box_iou(p_box, h_box) > 0.3: has_helmet = True

        for v_box in yolo_vests:
            if box_iou(p_box, v_box) > 0.3: has_vest = True

        status = "SAFE"
        missing = []

        # LOGIC: If YOLO missed something, Trigger SAM!
        if not has_helmet or not has_vest:
            print(f"   ‚ö†Ô∏è Person at {p_box[:2]} missing items. Triggering SAM...")
            sam_triggered = True

            # Run SAM only once globally if triggered
            if sam_helmet_masks is None:
                print("      ... Running SAM Helmet Scan")
                sam_helmet_masks = get_sam_masks(image_path, ["helmet", "hard hat"])
            if sam_vest_masks is None:
                print("      ... Running SAM Vest Scan")
                sam_vest_masks = get_sam_masks(image_path, ["vest", "safety vest"])

            # RE-CHECK with SAM masks
            # 1. Check Helmet
            if not has_helmet:
                # Look for mask overlap in head area
                head_zone = [p_box[0], p_box[1], p_box[2], int(p_box[1] + (p_box[3]-p_box[1])*0.35)]
                for m in sam_helmet_masks:
                    if m.shape[:2] != (h,w): m = cv2.resize(m, (w,h), interpolation=cv2.INTER_NEAREST)
                    roi = m[head_zone[1]:head_zone[3], head_zone[0]:head_zone[2]]
                    if np.sum(roi) > 0: has_helmet = True

            # 2. Check Vest
            if not has_vest:
                # Look for mask overlap in body area
                body_zone = [p_box[0], int(p_box[1] + (p_box[3]-p_box[1])*0.2), p_box[2], p_box[3]]
                for m in sam_vest_masks:
                    if m.shape[:2] != (h,w): m = cv2.resize(m, (w,h), interpolation=cv2.INTER_NEAREST)
                    roi = m[body_zone[1]:body_zone[3], body_zone[0]:body_zone[2]]
                    if np.sum(roi) > 0: has_vest = True

        # Final Status Calculation
        if not has_helmet: missing.append("Helmet")
        if not has_vest: missing.append("Vest")

        if missing: status = "UNSAFE"

        final_results.append({
            'bbox': p_box,
            'status': status,
            'missing': missing
        })

    return img_rgb, final_results, sam_triggered, (persons, yolo_helmets, yolo_vests)

# Execute
img_rgb, results, sam_ran, raw_yolo = run_smart_pipeline(IMAGE_PATH)

# ==========================================
# 4. CONDITIONAL VISUALIZATION
# ==========================================
fig, ax = plt.subplots(1, 2, figsize=(20, 10))

# --- PLOT 1: YOLO RAW ---
ax[0].imshow(img_rgb)
ax[0].set_title("Stage 1: YOLO Detection", fontsize=16, fontweight='bold')
ax[0].axis('off')

# Draw raw YOLO boxes
for p in raw_yolo[0]: # Persons
    ax[0].add_patch(plt.Rectangle((p[0], p[1]), p[2]-p[0], p[3]-p[1], lw=2, edgecolor='blue', facecolor='none'))
    ax[0].text(p[0], p[1]-5, "Person", color='blue', fontsize=8, fontweight='bold')
for h in raw_yolo[1]: # Helmets
    ax[0].add_patch(plt.Rectangle((h[0], h[1]), h[2]-h[0], h[3]-h[1], lw=2, edgecolor='yellow', facecolor='none'))
for v in raw_yolo[2]: # Vests
    ax[0].add_patch(plt.Rectangle((v[0], v[1]), v[2]-v[0], v[3]-v[1], lw=2, edgecolor='orange', facecolor='none'))

# --- PLOT 2: LOGIC RESULT ---
if not sam_ran:
    # SCENARIO: YOLO WAS PERFECT
    # Create a dummy "Skipped" image
    skip_img = np.zeros_like(img_rgb) + 240 # Light grey background
    ax[1].imshow(skip_img)
    ax[1].text(img_rgb.shape[1]//2, img_rgb.shape[0]//2, "SAM 3 SKIPPED\n(YOLO Detected All PPE)",
               ha='center', va='center', fontsize=20, color='green', fontweight='bold')
    ax[1].set_title("Stage 2: SAM Logic Bypass", fontsize=16, fontweight='bold')
    ax[1].axis('off')

    # Add Overlay to Plot 1 saying "CONFIRMED SAFE"
    ax[0].text(50, 50, "‚úÖ ALL SAFE (YOLO)", color='lime', fontsize=15, bbox=dict(facecolor='black', alpha=0.7))

else:
    # SCENARIO: SAM WAS TRIGGERED
    ax[1].imshow(img_rgb)
    ax[1].set_title("Stage 2: SAM 3 Rescue Triggered", fontsize=16, fontweight='bold')
    ax[1].axis('off')

    for item in results:
        x1, y1, x2, y2 = item['bbox']
        color = '#00FF00' if item['status'] == 'SAFE' else '#FF0000'

        # Draw Final Status Box
        ax[1].add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, lw=3, edgecolor=color, facecolor='none'))

        # Label
        label = "SAFE (Verified)" if item['status'] == 'SAFE' else f"MISSING: {', '.join(item['missing'])}"
        ax[1].text(x1, y1-10, label, color='white', fontsize=10, fontweight='bold',
                   bbox=dict(facecolor=color, alpha=0.9, edgecolor='none'))

plt.tight_layout()
plt.show()

--- CELL 24 ---
# @title Complete 3-Stage Safety Pipeline (Original -> YOLO -> SAM/Decision)
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO
from ultralytics.models.sam import SAM3SemanticPredictor
import sys

# ==========================================
# 1. SETUP
# ==========================================
IMAGE_PATH = '/content/Worker-working-without-safety-boots-hand-gloves-head-protection_Q320.jpg'
YOLO_WEIGHTS = '/content/best.pt'
SAM_WEIGHTS = "/content/sam3.pt"
SAM_IMAGE_SIZE = 1024

# Your Custom Class Mapping
TARGET_CLASSES = {
    'person': [6],      # Person ID
    'helmet': [1],      # Helmet ID
    'vest': [2],        # Vest ID
    'no_helmet': [7]    # No_Helmet ID (Explicit Unsafe)
}

print(f"üöÄ Initializing 3-Stage Pipeline...")

# Load YOLO
try:
    yolo_model = YOLO(YOLO_WEIGHTS)

    # Auto-update IDs if names match
    names = yolo_model.names
    name_to_id = {v: k for k, v in names.items()}
    if 'Person' in name_to_id: TARGET_CLASSES['person'] = [name_to_id['Person']]
    if 'helmet' in name_to_id: TARGET_CLASSES['helmet'] = [name_to_id['helmet']]
    if 'vest' in name_to_id: TARGET_CLASSES['vest'] = [name_to_id['vest']]
    if 'no_helmet' in name_to_id: TARGET_CLASSES['no_helmet'] = [name_to_id['no_helmet']]

    print(f"‚úÖ Active Class IDs: {TARGET_CLASSES}")

except Exception as e:
    sys.exit(f"‚ùå YOLO Error: {e}")

# Load SAM
overrides = dict(model=SAM_WEIGHTS, task="segment", mode="predict", conf=0.15)
sam_model = SAM3SemanticPredictor(overrides=overrides)

# ==========================================
# 2. LOGIC FUNCTIONS
# ==========================================
def box_iou(box1, box2):
    """Checks intersection/coverage"""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    inter_area = max(0, x2 - x1) * max(0, y2 - y1)
    if inter_area == 0: return 0

    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    return inter_area / box2_area

def run_sam_rescue(image_path, search_prompts, roi_box, img_h, img_w):
    """Runs SAM only on a specific ROI"""
    try:
        res = sam_model(image_path, text=search_prompts, imgsz=SAM_IMAGE_SIZE, verbose=False)
        if not res[0].masks: return False, None

        masks = [m.cpu().numpy().astype(np.uint8) for m in res[0].masks.data]

        for m in masks:
            if m.shape[:2] != (img_h, img_w):
                m = cv2.resize(m, (img_w, img_h), interpolation=cv2.INTER_NEAREST)

            roi = m[roi_box[1]:roi_box[3], roi_box[0]:roi_box[2]]
            if np.sum(roi) > 0:
                return True, m

    except Exception as e:
        print(f"      ‚ö†Ô∏è SAM Rescue Error: {e}")
    return False, None

# ==========================================
# 3. MAIN PIPELINE
# ==========================================
def run_pipeline(image_path):
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    h, w = img_rgb.shape[:2]

    # --- PHASE 1: YOLO ---
    print("üëÄ Phase 1: YOLO Scanning...")
    results = yolo_model.predict(image_path, conf=0.4, verbose=False)

    # Store Raw Detections for Stage 2 Figure
    raw_detections = {'person': [], 'helmet': [], 'vest': [], 'no_helmet': []}

    for box in results[0].boxes:
        cls = int(box.cls[0])
        coords = box.xyxy[0].cpu().numpy().astype(int)

        if cls in TARGET_CLASSES['person']: raw_detections['person'].append(coords)
        elif cls in TARGET_CLASSES['helmet']: raw_detections['helmet'].append(coords)
        elif cls in TARGET_CLASSES['vest']: raw_detections['vest'].append(coords)
        elif cls in TARGET_CLASSES['no_helmet']: raw_detections['no_helmet'].append(coords)

    print(f"   üëâ Detections: {len(raw_detections['person'])} Persons.")

    # --- PHASE 2: LOGIC & SAM ---
    final_output = []
    global_sam_triggered = False
    skip_reason = "YOLO Confident"

    for p_box in raw_detections['person']:
        # YOLO Matches
        yolo_helmet = False
        yolo_vest = False
        yolo_unsafe = False

        for eq in raw_detections['helmet']:
            if box_iou(p_box, eq) > 0.3: yolo_helmet = True
        for eq in raw_detections['vest']:
            if box_iou(p_box, eq) > 0.3: yolo_vest = True
        for eq in raw_detections['no_helmet']:
            if box_iou(p_box, eq) > 0.3: yolo_unsafe = True

        # DECISION TREE
        status = "UNKNOWN"
        missing = []
        action = "Skipped"
        found_mask = None

        if yolo_unsafe and yolo_vest:
            status = "UNSAFE"
            missing.append("Helmet")
            action = "Fast Unsafe"

        elif yolo_helmet and yolo_vest:
            status = "SAFE"
            action = "Fast Safe"

        elif yolo_helmet and not yolo_vest:
            print("   üîç Rescue: Looking for Vest...")
            global_sam_triggered = True
            skip_reason = "Missing Vest" # Update reason if triggered

            body_roi = [p_box[0], int(p_box[1] + (p_box[3]-p_box[1])*0.2), p_box[2], p_box[3]]
            found, mask = run_sam_rescue(image_path, ["vest", "safety vest"], body_roi, h, w)

            if found:
                status = "SAFE"
                found_mask = mask
                action = "SAM Found Vest"
            else:
                status = "UNSAFE"
                missing.append("Vest")
                action = "SAM Failed"

        elif yolo_vest and not yolo_helmet:
            print("   üîç Rescue: Looking for Helmet...")
            global_sam_triggered = True
            skip_reason = "Missing Helmet"

            head_roi = [p_box[0], p_box[1], p_box[2], int(p_box[1] + (p_box[3]-p_box[1])*0.4)]
            found, mask = run_sam_rescue(image_path, ["helmet", "hard hat"], head_roi, h, w)

            if found:
                status = "SAFE"
                found_mask = mask
                action = "SAM Found Helmet"
            else:
                status = "UNSAFE"
                missing.append("Helmet")
                action = "SAM Failed"

        else:
             print("   üîç Rescue: Full Scan...")
             global_sam_triggered = True
             skip_reason = "Missing All Gear"

             # Full Scan logic (omitted for brevity, same as before)
             status = "UNSAFE" # Default if rescue omitted
             missing = ["Helmet", "Vest"]
             action = "Full Search"

        final_output.append({
            'bbox': p_box,
            'status': status,
            'missing': missing,
            'action': action,
            'mask': found_mask
        })

    return img_rgb, raw_detections, final_output, global_sam_triggered, skip_reason

# Execute
img_rgb, raw_yolo, results, sam_ran, reason = run_pipeline(IMAGE_PATH)

# ==========================================
# 4. VISUALIZATION (3 FIGURES)
# ==========================================
fig, ax = plt.subplots(1, 3, figsize=(24, 8))

# --- FIG 1: ORIGINAL ---
ax[0].imshow(img_rgb)
ax[0].set_title("Stage 1: Original Image", fontsize=16, fontweight='bold')
ax[0].axis('off')

# --- FIG 2: YOLO RAW ---
ax[1].imshow(img_rgb)
ax[1].set_title("Stage 2: YOLO Detection (The Scout)", fontsize=16, fontweight='bold')
ax[1].axis('off')

# Draw Raw Detections
for p in raw_yolo['person']:
    ax[1].add_patch(plt.Rectangle((p[0], p[1]), p[2]-p[0], p[3]-p[1], lw=2, edgecolor='blue', facecolor='none'))
    ax[1].text(p[0], p[1]-5, "Person", color='blue', fontsize=9, fontweight='bold')

for h in raw_yolo['helmet']:
    ax[1].add_patch(plt.Rectangle((h[0], h[1]), h[2]-h[0], h[3]-h[1], lw=2, edgecolor='lime', facecolor='none'))
    ax[1].text(h[0], h[1]-5, "Helmet", color='lime', fontsize=8)

for v in raw_yolo['vest']:
    ax[1].add_patch(plt.Rectangle((v[0], v[1]), v[2]-v[0], v[3]-v[1], lw=2, edgecolor='orange', facecolor='none'))
    ax[1].text(v[0], v[1]-5, "Vest", color='orange', fontsize=8)

for nh in raw_yolo['no_helmet']:
    ax[1].add_patch(plt.Rectangle((nh[0], nh[1]), nh[2]-nh[0], nh[3]-nh[1], lw=2, edgecolor='red', facecolor='none'))
    ax[1].text(nh[0], nh[1]-5, "No Helmet", color='red', fontsize=8)

# --- FIG 3: SAM / FINAL DECISION ---
if not sam_ran:
    # SAM SKIPPED MODE
    # Darken image to make text pop
    dark_img = (img_rgb * 0.4).astype(np.uint8)
    ax[2].imshow(dark_img)
    ax[2].set_title("Stage 3: SAM Verification (The Judge)", fontsize=16, fontweight='bold')

    # Big Center Text
    ax[2].text(img_rgb.shape[1]//2, img_rgb.shape[0]//2,
               f"SAM 3 SKIPPED\n({reason})",
               ha='center', va='center', fontsize=22, color='white', fontweight='bold',
               bbox=dict(facecolor='#00AA00', alpha=0.8, edgecolor='white', boxstyle='round,pad=1'))
    ax[2].axis('off')

else:
    # SAM ACTIVE MODE
    ax[2].imshow(img_rgb)
    ax[2].set_title(f"Stage 3: SAM Rescue Triggered ({reason})", fontsize=16, fontweight='bold')
    ax[2].axis('off')

    for item in results:
        x1, y1, x2, y2 = item['bbox']
        color = '#00FF00' if item['status'] == 'SAFE' else '#FF0000'

        # Draw Box
        ax[2].add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, lw=3, edgecolor=color, facecolor='none'))

        # Draw Mask if SAM found one
        if item['mask'] is not None:
             masked_region = np.ma.masked_where(item['mask'] == 0, item['mask'])
             ax[2].imshow(masked_region, cmap='spring', alpha=0.5, interpolation='none')

        # Label
        label = f"{item['status']}" #\nAction: {item['action']}
        ax[2].text(x1, y1-20, label, color='white', fontsize=9, fontweight='bold',
                   bbox=dict(facecolor=color, alpha=0.8))

plt.tight_layout()
plt.show()
